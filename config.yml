verbose: true
device: "mps"

llama_config:
  model_name: "meta-llama/Meta-Llama-3-8B-Instruct"
  system_prompt: "You are a helpful assistant"
  max_num_historical_messages: 6
  model_size: "8B"                 # Model size is 70B
  max_tokens: 256                   # Maximum tokens for generation
  temperature: 0.7                  # Sampling temperature
  top_p: 0.9                        # Top-p sampling

sd_config:
  root_models_path: &root_models_path /Users/ahestevenz/.cache/huggingface/hub
  root_output_dir: /Users/ahestevenz/Desktop/hola
  model_type: "civitai"               # Using SDXL model
  num_frames: 30                    # Number of frames for video (optional)
  fps: 10                           # FPS for video (optional)
